{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 1: Linear Regression\n",
    "\n",
    "This notebook is based on the [Coursera Machine Learning](https://www.coursera.org/learn/machine-learning) course by Andrew Ng. We believe it is the best course for people who are keen to make their first steps in Machine Learning.\n",
    "\n",
    "The course is taught in Octave / MATLAB, but Python is much more broadly used in the industry and academia. The Python code in this tutorial is based on some work by [David Kaleko](https://github.com/kaleko). \n",
    "\n",
    "Explanations of the model, and any mistakes herein, are of our own doing. If you're unsure about anything, feel free to ask us during the session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the libraries with the necessary functionality for ML\n",
    "# in Python and for vanilla ML, that almost always includes NumPy\n",
    "\n",
    "# the following line allows us to output plots in the notebook\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # plotting library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of supervised learning is to train a model to predict a label $y$ given an input $x = (x_1, x_2, \\dotsc, x_n)$, where $n$ is the number of input features. For example, $y$ as the target variable can be the price of a house, and $x$ is a vector consisting of $n$ features of the house, such as: the number of bedrooms, proximity to public transport, age of house, etc... Our task then is to build a model that can take these input features of the house and predict the price that the house will be sold at. \n",
    "\n",
    "To train a supervised machine-learning algorithm, you would need a bunch of labelled examples $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dotsc, (x^{(m)}, y^{(m)})\\}$ which we call the **training data**, where $m$ is the number of datapoints. Using the above example, we would need a dataset containing the sold price of $m$ houses, along with the $n$ features for each of those houses.\n",
    "\n",
    "A supervised machine learning model typically assumes that there exists a mapping from inputs to outputs, which we can represent as\n",
    "\n",
    "$$ y = h_{\\theta}(x) + \\epsilon $$\n",
    "\n",
    "where $h_{\\theta}(x)$ is called a hypothesis and $\\epsilon$ (pronounced $epsilon$) is a noise term that represents any error that cannot be accounted for by the hypothesis $h$. $\\theta$ (pronounced $theta$) are the model parameters which we will want to tweak to make the predictions as accurate as possible (you will see what we mean in a bit).\n",
    "\n",
    "Linear regression is one of the simplest ML models. It assumes that the mapping from $x$ to $y$ is linear in parameters (i.e. it is composed of the sum of model parameters):\n",
    "\n",
    "$$ y = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n + \\epsilon$$\n",
    "\n",
    "where $\\theta_1 x_1$ is $\\theta_1$ times $x_1$, $x = (x_1, x_2, \\dotsc, x_n)$ are the input variables for one data sample and $\\theta = (\\theta_0, \\theta_1, \\theta_2, \\dotsc, \\theta_n)$ are the model parameters to be learned.\n",
    "\n",
    "Intuitively, the parameter $\\theta_n$ in $\\theta_n x_n$ denotes the slope or how much $y$ is expected to change with each unit change in $x_n$. $\\theta$ can be either positive or negative, e.g. house price is expected to increase by $\\theta_{bed}$ for every extra bedroom, or it is expected to decreased by $\\theta_{transport}$ for every additional meter from a public transport link.\n",
    "\n",
    "Note that we have a $\\theta_n$ for each $x_n$ but we have one more additional term, $\\theta_0$, which we call the bias. This can be interpreted as the *intercept* term, or what $y$ is expected to be in absence of any additional knowledge about $x_n$.\n",
    "\n",
    "It is important to note that linearity is in parameters $\\theta$, *not* the variables $x$. This means that a linear regression permits polynomials or interactions of original variables, for example:\n",
    "\n",
    "$$ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_2 + \\theta_4 x_1x_2 + \\epsilon$$\n",
    "\n",
    "In theory, this permits the model to learn non-linear relationships between $y$ and (non-transformed) $x$. However, these non-linear relationships and interaction effects are not known a-priori. In practice, we tend to use the simplest model with no higher-order terms.\n",
    "\n",
    "In general linear regression is not the most predictive model out of the box. Nonetheless, it is a very useful baseline in many contexts (and in industry) and benefits from the ease of interpretation and development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple linear regression model in one variable. You will implement linear regression with one\n",
    "variable to predict profits for a food truck. \n",
    "\n",
    "Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet.\n",
    "The chain already has trucks in various cities and you have a dataset containing the profits and population size for $m$ cities.\n",
    "The training data can be represented as $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dotsc, (x^{(m)}, y^{(m)})\\}$ where $x^{(i)}$ represents the population (in 10k increment) and $y^{(i)}$ represents profit (in \\$10k increment) of the $i$ th city.\n",
    "\n",
    "You would like to use this data to help you select which city to expand to next.\n",
    "\n",
    "The file *ex1data1.txt* contains the dataset for our linear regression problem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a\n",
    "loss.\n",
    "\n",
    "\n",
    "Consider output of the hypothesis $h_{\\theta}$\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1 x. $$\n",
    "\n",
    "\n",
    "Let's try to interpret this equation. It assumes that the relationship between $y$ and $x$ is linear -- meaning that if we increase $x$ by 1, $h_{\\theta}(x)$ will increase by $\\theta_1$ and if we increase $x$ by $47.1$, $h_{\\theta}(x)$ will increase $47.1 \\cdot \\theta_1$, or $47.1$ times $\\theta_1$. The term $\\theta_0$ allows to shift the linear dependency between $h_{\\theta}(x)$ and $x$ by a constant. When $x = 0$, $h_{\\theta}(x)$ predicts $y$ to be exactly equal to $\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading the Data\n",
    "\n",
    "In machine learning, we usually represent the data $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dotsc, (x^{(m)}, y^{(m)})\\}$ as a matrix $X$ and a vector $y$ where\n",
    "\\begin{align}\n",
    "    X &=\n",
    "        \\begin{bmatrix}\n",
    "            x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "            x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\\\\n",
    "        \\end{bmatrix} &&\\text{and}\\\\\n",
    "    y &= \n",
    "        \\begin{bmatrix}\n",
    "            y^{(1)} \\\\\n",
    "            y^{(2)} \\\\\n",
    "            \\vdots \\\\\n",
    "            y^{(m)}\n",
    "        \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "In the one dimensional case the matrix $X$ is just\n",
    "\\begin{align}\n",
    "    X &=\n",
    "        \\begin{bmatrix}\n",
    "            x^{(1)} \\\\\n",
    "            x^{(2)} \\\\\n",
    "            \\vdots \\\\\n",
    "            x^{(m)}\n",
    "        \\end{bmatrix}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.592  ]\n",
      " [ 9.1302 ]\n",
      " [13.662  ]\n",
      " [11.854  ]\n",
      " [ 6.8233 ]\n",
      " [11.886  ]\n",
      " [ 4.3483 ]\n",
      " [12.     ]\n",
      " [ 6.5987 ]\n",
      " [ 3.8166 ]\n",
      " [ 3.2522 ]\n",
      " [15.505  ]\n",
      " [ 3.1551 ]\n",
      " [ 7.2258 ]\n",
      " [ 0.71618]\n",
      " [ 3.5129 ]\n",
      " [ 5.3048 ]\n",
      " [ 0.56077]\n",
      " [ 3.6518 ]\n",
      " [ 5.3893 ]\n",
      " [ 3.1386 ]\n",
      " [21.767  ]\n",
      " [ 4.263  ]\n",
      " [ 5.1875 ]\n",
      " [ 3.0825 ]\n",
      " [22.638  ]\n",
      " [13.501  ]\n",
      " [ 7.0467 ]\n",
      " [14.692  ]\n",
      " [24.147  ]\n",
      " [-1.22   ]\n",
      " [ 5.9966 ]\n",
      " [12.134  ]\n",
      " [ 1.8495 ]\n",
      " [ 6.5426 ]\n",
      " [ 4.5623 ]\n",
      " [ 4.1164 ]\n",
      " [ 3.3928 ]\n",
      " [10.117  ]\n",
      " [ 5.4974 ]\n",
      " [ 0.55657]\n",
      " [ 3.9115 ]\n",
      " [ 5.3854 ]\n",
      " [ 2.4406 ]\n",
      " [ 6.7318 ]\n",
      " [ 1.0463 ]\n",
      " [ 5.1337 ]\n",
      " [ 1.844  ]\n",
      " [ 8.0043 ]\n",
      " [ 1.0179 ]\n",
      " [ 6.7504 ]\n",
      " [ 1.8396 ]\n",
      " [ 4.2885 ]\n",
      " [ 4.9981 ]\n",
      " [ 1.4233 ]\n",
      " [-1.4211 ]\n",
      " [ 2.4756 ]\n",
      " [ 4.6042 ]\n",
      " [ 3.9624 ]\n",
      " [ 5.4141 ]\n",
      " [ 5.1694 ]\n",
      " [-0.74279]\n",
      " [17.929  ]\n",
      " [12.054  ]\n",
      " [17.054  ]\n",
      " [ 4.8852 ]\n",
      " [ 5.7442 ]\n",
      " [ 7.7754 ]\n",
      " [ 1.0173 ]\n",
      " [20.992  ]\n",
      " [ 6.6799 ]\n",
      " [ 4.0259 ]\n",
      " [ 1.2784 ]\n",
      " [ 3.3411 ]\n",
      " [-2.6807 ]\n",
      " [ 0.29678]\n",
      " [ 3.8845 ]\n",
      " [ 5.7014 ]\n",
      " [ 6.7526 ]\n",
      " [ 2.0576 ]\n",
      " [ 0.47953]\n",
      " [ 0.20421]\n",
      " [ 0.67861]\n",
      " [ 7.5435 ]\n",
      " [ 5.3436 ]\n",
      " [ 4.2415 ]\n",
      " [ 6.7981 ]\n",
      " [ 0.92695]\n",
      " [ 0.152  ]\n",
      " [ 2.8214 ]\n",
      " [ 1.8451 ]\n",
      " [ 4.2959 ]\n",
      " [ 7.2029 ]\n",
      " [ 1.9869 ]\n",
      " [ 0.14454]\n",
      " [ 9.0551 ]\n",
      " [ 0.61705]]\n"
     ]
    }
   ],
   "source": [
    "datafile = 'data/ex1data1.txt'\n",
    "cols = np.loadtxt(datafile, delimiter=',', usecols=(\n",
    "    0, 1), unpack=True)  # Read in comma separated data\n",
    "# Form the usual \"X\" matrix and \"y\" vector\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "print(y)\n",
    "m = y.size  # number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Plotting the Data\n",
    "\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population).\n",
    "\n",
    "We focus on the inner workings of machine learning algorithms and therefore you do not need to fill in anything for plotting and model diagnostics. Simply execute the cells with matplotlib code to see what you have achieved so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Population of City in 10,000s')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAFzCAYAAADv+wfzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df5hcZX338c/XCSGwyw+JEhGQUPlRKIkbCRsQtRttE5amIlarPKJYtdSrDyJSi6BULNFWwEdatdXHFhRbSzAUCs3FNuGxLoRfCQkbEzAKlAuU3zWgOEs2IcP3+eM+052dzOyc2Z1zzsyc9+u65pqZM+fM3HNnsvOd+/6e723uLgAAACTvFVk3AAAAIC8IvAAAAFJC4AUAAJASAi8AAICUEHgBAACkhMALAAAgJTOybkAcr3rVq3zu3LmJvsbo6Kh6enoSfY1ORx/FQz/FQz/FQz/FQz/FQz811oo+2rhx4y/c/dW1HuuIwGvu3LnasGFDoq8xPDysgYGBRF+j09FH8dBP8dBP8dBP8dBP8dBPjbWij8zssXqPJTbVaGaHmtkPzWyrmT1gZp+Itn/ezJ4ws03R5dSk2gAAANBOkhzx2iXpz9z9PjPbR9JGM7s1euxKd/9ygq8NAADQdhILvNz9KUlPRbd/bWZbJR2c1OsBAAC0O0tjrUYzmyvpdknHSTpf0ockvSBpg8Ko2PM1jjlb0tmSNGfOnONXrFiRaBuLxaJ6e3sTfY1ORx/FQz/FQz/FQz/FQz/FQz811oo+Wrx48UZ3X1jrscQDLzPrlXSbpC+6+w1mNkfSLyS5pOWSDnL3D0/2HAsXLnSS67NHH8VDP8VDP8VDP8VDP8VDPzXWouT6uoFXonW8zGwPSf8q6XvufoMkufsz7l5y95cl/YOk/iTbAAAA0C6SPKvRJF0laau7f6Vi+0EVu50u6f6k2gAAANBOkjyr8WRJH5C0xcw2Rds+I+kMM+tTmGp8VNKfJNgGAACAtpHkWY13SLIaD92S1GsCAAC0s46oXA8AADAdpZI0NCSNjEgLFkiDg1KhkH47CLwAAEBXK5WkpUuldeuk0VGpp0datEhavTr94CvRsxoBAACyNjQUgq5iUXIP1+vWhe1pI/ACAABdbWQkjHRVGh2VNm2qvX+SCLwAAEBXW7AgTC9W6umR+vrSbwuBFwAA6GqDgyGnq7dXMgvXixaF7WkjuR4AAHS1QiEk0g8NhenFvj7OagQAAEhMoSAtWxYuWWKqEQAAICUEXgAAACkh8AIAAEgJgRcAAEBKCLwAAABSQuAFAACQEgIvAACAlBB4AQAApITACwAAICVUrgcAIGGlUliuZmQkLNic1XI1yB6BFwAACSqVpKVLpXXrpNFRqacnLNC8ejXBVx4x1QgAQIKGhkLQVSxK7uF63bqwHflD4AUAQIJGRsJIV6XRUWnTpmzag2wReAEAkKAFC8L0YqWeHqmvL5v2IFsEXgAAJGhwMOR09fZKZuF60aKwHflDcj0AAAkqFEIi/dBQmF7s6+Osxjwj8AIAIGGFgrRsWbgg35hqBAAASAmBFwAAQEqYagQAIMeoqp8uAi8AAHKKqvrpY6oRAICcoqp++gi8AADIKarqp4/ACwCAnKKqfvoIvAAAyCmq6qeP5HoAAHKKqvrpI/ACACDHqKqfLgIvAAAgiZpeaSDwAgAA1PRKCcn1AACAml4pIfACAADU9EoJgRcAAKCmV0oIvAAAADW9UkJyPQAAoKZXSgi8AACAJGp6pYGpRgAAgJQQeAEAAKSEwAsAACAliQVeZnaomf3QzLaa2QNm9olo+wFmdquZPRRdvzKpNgAAALSTJEe8dkn6M3c/RtKJkv63mR0r6UJJP3D3IyX9ILoPAADQ9RILvNz9KXe/L7r9a0lbJR0s6TRJ10S7XSPpnUm1AQAAoJ2Yuyf/ImZzJd0u6ThJP3P3/Ssee97dd5tuNLOzJZ0tSXPmzDl+xYoVibaxWCyqt7c30dfodPRRPPRTPPRTPPRTPPRTPPRTY63oo8WLF29094W1Hku8jpeZ9Ur6V0nnufsLZhbrOHf/lqRvSdLChQt9YGAgsTZK0vDwsJJ+jU5HH8VDP8VDP8VDP8VDP8VDPzWWdB8lelajme2hEHR9z91viDY/Y2YHRY8fJOnZJNsAAADQLpI8q9EkXSVpq7t/peKhmyWdFd0+S9JNSbUBAACgnSQ51XiypA9I2mJmm6Jtn5H0JUnfN7OPSPqZpPck2AYAAIC2kVjg5e53SKqX0PX2pF4XAACgXVG5HgAAICUEXgAAACkh8AIAAEgJgRcAAEBKCLwAAABSQuAFAACQEgIvAACAlBB4AQAApITACwAAICUEXgAAACkh8AIAAEgJgRcAAEBKCLwAAABSQuAFAACQkhlZNwAAAORPqSQNDUkjI9KCBdLgoFQoZN2q5BF4AQCAVJVK0tKl0rp10uio1NMjLVokrV7d/cEXU40AACBVQ0Mh6CoWJfdwvW5d2N7tCLwAAECqRkbCSFel0VFp06Zs2pMmphozltc5bgBAfi1YEKYXi8XxbT09Ul9fdm1KC4FXhvI8xw0AyK/BwfB9V/39NziYdcuSR+CVoco5bmniHPeyZdm2DQCApBQKYZBhaChML/b15WfGh8ArQ5PNcRN4AQC6WaEQvuvy9n1Hcn2GynPclfIyxw0AQB4ReGWoPMfd2yuZheu8zHEDAJBHTDVmKM9z3ACQN5zFDonAK3N5neMGgDzhLHaUMdUIAEDC8lypHRMReAEAkLA8V2rHRAReAAAkjLPYUUbgBQBAwjiLHWUk1wMAkDDOYkcZgRcAACngLHZITDUCAACkhsALAAAgJQReAAAAKSHwAgAASAmBFwAAQEoIvAAAAFJC4AUAAJASAi8AAICUEHgBAACkhMr1ANCkUiks/TIyEhY/ZukXAHEReAFAE0olaelSad06aXRU6ukJix2vXk3wBaAxphoBoAlDQyHoKhYl93C9bl3YDgCNEHgBQBNGRsJIV6XRUWnTpmzaA6CzEHgBQBMWLAjTi5V6eqS+vmzaA6CzJBZ4mdnVZvasmd1fse3zZvaEmW2KLqcm9foAkITBwZDT1dsrmYXrRYvCdgBoJMnk+u9I+rqk71Ztv9Ldv5zg6wJAYgqFkEg/NBSmF/v6OKsRQHyJBV7ufruZzU3q+QEgK4WCtGxZuABAM7LI8TrHzDZHU5GvzOD1AQAAMmHuntyThxGvVe5+XHR/jqRfSHJJyyUd5O4frnPs2ZLOlqQ5c+Ycv2LFisTaKUnFYlG9vb2Jvkano4/ioZ/iaVU/lUrS+vWz9dBDvTryyKL6+7d11bQfn6d46Kd46KfGWtFHixcv3ujuC2s9lmrgFfexagsXLvQNGza0unkTDA8Pa2BgINHX6HT0UTz0Uzyt6Kc8FDPl8xQP/RQP/dRYK/rIzOoGXqlONZrZQRV3T5d0f719AaARipkC6DRJlpO4VtLdko42s8fN7COSLjezLWa2WdJiSZ9M6vUBdD+KmQLoNE2f1RglxB/q7psn28/dz6ix+apmXw8A6ikXMy0Wx7dRzBRAO4s14mVmw2a2r5kdIOlHkr5tZl9JtmkAMDmKmQLoNHFHvPZz9xfM7KOSvu3ul0TThQCQGYqZAug0cQOvGVFi/B9K+myC7QGAplDMFEAniZtcf6mk1ZIedvd7zew3JD2UXLMAAAC6T6wRL3dfKWllxf1HJP1BUo0CAADoRg0DLzNbKumdkg5WqDj/pKSb3P0/Em4bAABAV5k08DKzv5F0lKTvSno82nyIpHPNbNDdP5Fw+5BjpVJImh4ZCWUDSJoGAHS6RiNep7r7UdUbzew6SQ9KIvBCIvKwFAwAIH8aJdePmVl/je0nSBpLoD2AJJaCQQi+V62Sli8P16VS1i0CgOlrNOL1IUnfMLN9ND7VeKikF6LHgERMthQMZQO6HyOeALrVpIGXu98naZGZvUYhud4kPe7uT6fROOQXS8HkW+WIpzRxxJPAG0Ana1jHy8xM0mHR5VBJh0XbgMSwFEy+sfg1gG7V6KzGJZL+XqFY6hPR5kMkHWFmf+ruaxJuH3KKpWDyjRFPAN2qUY7X30r6HXd/tHKjmR0u6RZJxyTULoClYHKsPOJZnePFiCeATtco8Jqh8aT6Sk9I2qP1zQEARjwBdK9GgdfVku41sxWSfh5tO1TS+yRdlWTDAOQbI54AulGjsxr/2sxukvQOSScpOqtR0vvd/ccptA8AAKBrNFyrMQqwCLIAAACmadJyEma2n5l9ycx+YmbbosvWaNv+aTUSAACgGzSq4/V9Sc9LGnD32e4+W9JiSb+UtDLpxgEAAHSTRoHXXHe/rLJSvbs/7e5fkvS6ZJsGAADQXRoFXo+Z2QVmNqe8wczmmNmnNX6WIwAAAGJoFHi9V9JsSbeZ2XNm9pykYUkHSPrDhNsGAADQVRqVk3he0qejC9AVSqVQmHNkJCxNQ2FOAEBaGpaTqMXMTpP0tLuva3F7gESVStLSpbsvRbN6NcEXslfrRwGA7jKlwEvSIknzzGyGu/OnAYlq5QjV0FAIusqLLxeL4f7QEBXSka16PwouuijrlgFopSkFXu7+mVY3BMlr9ym2er/2WzlCNTISnqfS6GhYD5DAC1mq96Ng/frZevvbs20bgNZpGHiZ2X6STpF0sCSX9KSk1e7+y4TbhhZq9ym2eu37+MdbO0K1YEF47vLzSeF+X19r3gcwVfV+FDz8cG82DQKQiEaV6z8o6T5JA5L2ltSjUEB1Y/QYOkTlr2n3iQFMO6jXvpUr649QTcXgYAjoensls3C9aBG5NMhe+UdBpZ4e6YgjirUPANCRGo14fVbS8dWjW2b2SknrJH03qYahtdp9iq1e+8xaO0JVKIRRvqGh8N77+tpvyhX5VP5RUD3q29+/LeumAWihRoGXKUwvVns5egwdot2n2Oq1793vlp56avcvo+mMUBUKIdhsh4ATKKv3o2Dt2qxbBqCVGgVeX5R0n5mt0Xil+tdJ+l1Jy5NsGFqr3q/pdpliq9e+coDECBXygB8FQPdrVED1GjO7WdJSheR6U6hcf1FUXBUdot2n2Bq1jy8jAEA3aHhWYxRgrUihLUhYu/+abvf2AQAwXY3WaqzLzLa0siEAAADdbtIRLzN7V72HJL2m9c0BAADoXo2mGq+T9D3VPrNxVuubAwAA0L0aBV6bJX3Z3e+vfsDMfieZJgEAAHSnRjle50l6oc5jp7e4LQAAAF2tUTmJuqX73H1D65sDAADQvRqe1WhmB5pZT3R7LzP7rJl9ycwOSr55AAAA3SNOOYkVkmZHt/9S0hGSnpf0L0k1CgAAoBtNGniZ2VmSXi9pILr9XkkbJD0t6TAz+6CZzU++mQAAAJ2v0VmNw5K2S9oqaT9Jz0j6d4U6XudEj/8queYBAAB0j0bJ9Y+Z2d9KWiVpD0kfdPefmdnrJP3C3X+WRiMBAAC6QZy1Gr9hZv8k6WV3fzHavE3SGYm2DAAAoMvEWqvR3YsVQZfcfdTdfznZMWZ2tZk9a2b3V2w7wMxuNbOHoutXTr3pALpZqSStWiUtXx6uS6WsWwQA0zflRbJj+I6kU6q2XSjpB+5+pKQfRPcBYIJSSVq6VDrjDOmSS8L10qUEXwA6X2KBl7vfLum5qs2nSbomun2NpHcm9foAOtfQkLRunVQsSu7het26sB0AOlmSI161zHH3pyQpuj4w5dcH0AFGRqTR0YnbRkelTZuyaQ8AtIq5e+OdzN4l6TKFQMmii7v7vg2OmytplbsfF93/pbvvX/H48+5eM8/LzM6WdLYkzZkz5/gVK1bEeT9TViwW1dvbm+hrdDr6KB76KZ7J+unuu2dr+fJjtH37+Pk/s2bt0uc+t1UnnbQtrSa2BT5P8dBP8dBPjbWijxYvXrzR3RfWeixu4PWwpN93963NvHCNwOunkgbc/aloyaFhdz+60fMsXLjQN2xIdmnI4eFhDQwMJPoanY4+iod+imeyfirneK1bF0a6enqkRYuk1aulQiHddmaNz1M89FM89FNjregjM6sbeDUsJxF5ptmgq46bJZ0l6UvR9U0teE4AXaZQCEHW0FCYXuzrkwYH8xd0Aeg+cQOvDWZ2naR/k7SjvNHdb6h3gJldK2lA0qvM7HFJlygEXN83s49I+pmk90yx3QC6XKEgLVsWLgDQLeIGXvtKelHSkoptLqlu4OXu9Qqsvj3mawIAAHSVWIGXu/9R0g0BAADodpMGXmZ2gbtfbmZfUxjhmsDdz02sZR2gVAo5KCMj0oIF5KAAAIDJNRrxKifUJ3tKYQfirCugvfHDCEA7mjTwcvd/j66vmWy/PKqsrC1NrKxNMjCQLX4YAWhXaVeu7xpU1gbaF0sOAWhXBF5TtGBB+BVdqacn1BsCkC1+GAFoV7ECLzM7Oc62PBkcDFMXvb2SWbhetChsB5AtfhgBaFdx63h9TdIbY2zLDSprA+2r/MOoOseLH0YAstaonMRJkt4k6dVmdn7FQ/tKyn2IQWVtoD3xwwhAu2o04jVTUm+03z4V21+Q9O6kGgUA08UPIwDtqFE5idsk3WZm33H3x1JqEwAAQFdqNNX4N+5+nqSvm1mtyvXvSKxlAAAAXabRVON3o+svJ90QAACAbtco8LpC0tslnerun06hPQAAAF2rUeB1kJn9tqR3mNkKSVb5oLvfl1jLAGSCNQ4BIDmNAq/PSbpQ0iGSvlL1mEt6WxKNApAN1jgEgGQ1OqvxeknXm9lfuPvylNoEdJ1OGUVi8XcASFasyvXuvtzM3iHprdGmYXdflVyzgO7RSaNIk61xSOAFANMXd63Gv5b0CUk/ji6fiLYBaKByFMl94ihSu2GNQwBIVqzAS9LvSfpdd7/a3a+WdEq0DciVUklatUpavjxcl0qNj5lsFKndsPg7ACQr7iLZkrS/pOei2/sl0BagrU11yrA8ilTOm5LadxSJNQ4BIFlxA6+/ljRiZj9UKCnxVkkXJdYqoA1NNfG8PIpUHbC16ygSaxwCQHIaBl5mZpLukHSipBMUAq9Pu/vTCbcNOdTOZ/9NNfGcUSQAQFnDwMvd3cz+zd2Pl3RzCm3qOu0cTLSTdj/7bzpThowiAQCk+FON95jZCe5+b6Kt6ULtHky0k3avIdVuU4YE9ADQeeIGXoslfczMHpU0qjDd6O4+P6mGdYt2DybaSbvXkGqnKUMCegDoTHEDrzZNA25/7R5MtJNOOPuvXaYMCegBoDNNWsfLzGaZ2XmS/lyhdtcT7v5Y+ZJKCztcrYKUe+/dXsFEu0i6htRUanC1q06qDQYAGNdoxOsaSS9JWqsw6nWsQgV7xLRkibTHHhO3zZwZtneacuBSL6doujlHSU7ltcPUXCtzsjphdBAAsLtGgdex7j5PkszsKknrk29Sd1mzRtq5c+K2nTvD9manhLJMpi6VpAsumK8HH6wduLQqsElqKi+LqbnKf6/586WvflVav741gV+7JfoDAOJpFHi9VL7h7rtCSS80Y2REevHFidtefLH5HK+sR2yGhqStW/fV9u3hfnXg0mxgk3YQmXauXfW/1557Si+9ND69Od3Ar50S/QEA8TUKvN5gZi9Et03SXtH98lmN+ybaui7QqimhrJOpR0aksbGJ3+qVgUszgU0WQWTaU3PV/15jY7vvM93Ar10S/QEA8U2aXO/uBXffN7rs4+4zKm4TdMXQqoTxrJOpFyyQZs2amI1eGbjUOomgXmBTGZS4Twwik5L24s+1/r2qkZMFAPnTzCLZmIJWTQllnUw9OCgdc8wLevDBA2rmFDWTc5RFiY20p+Zq/XsVCuFEix07yMkCgLwi8EpBK6aEsk6mLhSkyy/frO3bB2oGLs0ENlkFkWlOzdX69+rvl849V9qyhZwsAMgrAq8O0Q7J1I0Cl7iBTdZBZBom+/c67bSsWwcAyAqBVwepDGw6eZ2+dggi00DyOwCgGoFXB8q6tEQrEJQAAPJo0rMa0Z6yOCsQAABMH4GXwgjS3XfP7pg1/LIuLdFq3bSGIgAAk8n9VGN52u6uu47R2FhnTNtlXVqilbph2hQAgLhyP+JVnrbbvn1Gx0zbpV0MNElMmwIA8iT3I15ZFPOMq96Zi910VmA79z8AAK2W+8CrXaft6k3B3XKLtGbNeDB20UXNB1ztVIqinfq/nfola/QFACQj94FXedruzjt3aceOGW1TzLPWoth33CGdcIL0yCNTz4dqt5yqdimm2m79kiX6AgCSk/vAqzxtd/nlW1UqzWubabtaU3A7doTlZtzD/cp8qLjTcrUCumafo5WmO23aqpGZduuXLNEXAJCc3AdeUvii7u/fpu3bwxe4lH3wNX9+SJwvB1ll1febzYdqx5yqqRZTbeXITDv2S1boCwBITiaBl5k9KunXkkqSdrn7wizaUVYqSRdcMF8PPtheUyvVQVYtzeZDTTenqp1yf1o5MtNOuWZZoy8AIDlZlpNY7O59WQddUvii3rp137YqabB5c+3Ay0yaNWvqZSRqlaLo7w8BVaMCpuURpjPOkC65JFwvXZpdwdNWFpLtphId00VfAEBymGpU+AIfG5s4bJP11MqCBeELr3LUQZLmzZMuvTTkek0lH606p2rePOmrX5XOPLPxaN/69bPbKvenlSMz3VSiY7roCwBITlaBl0taY2Yu6f+6+7eqdzCzsyWdLUlz5szR8PBwYo2ZMWO29tzzGI2NjXfHnnvuUqGwVcPD2xJ73cnstZd01FHz9eMf76uxsYJmzHAddtiorrhio2bOlN785rDf2rVTe/7e3vAcd989W3fffYy2bw/vvVgMZ3hefvlWnXTSxPf+wAMHaXTUJdn/bBsddd1446Pq7X1sag2Zhso+2rGjoD33LOmoo17QXntt1lQ/LuV+kabet8ViMdHPa1pa0ReT6ZZ+Shr9FA/9FA/91FjSfZRV4HWyuz9pZgdKutXMfuLut1fuEAVj35KkhQsX+sDAQGKNectbpOuvf04PPnhAxajPDF1wwbzUz66rtH595aiDaXBwHxUKA9N70ipr10pjYxO37dgxQ6XSPFV3+d13b1FPj1WNMJlOP/1wDQwc3tJ2xTWxj2ZocPCAlvdRs4aHh5Xk57Vb0E/x0E/x0E/x0E+NJd1HmQRe7v5kdP2smd0oqV/S7ZMflZxCQbr88s3avn1gyiUNkqh7NNWz/ZrRzHRdf/+22DW30krCT6OPAABoldQDLzPrkfQKd/91dHuJpEvTbke16XyBd3Ldo2YKmMbN/aEAJwAAtWUx4jVH0o1mVn79f3H3/8igHS3TyXWP4gRT5dGrG244TMVieHyy99XJgSgAAElKPfBy90ckvSHt1221yqm0l17q7LpH9Ub7SqVQXuL886Unn5TGxuZq5cpQfuLcc0PJi1rTiK0KRNupZhgAAK1AOYkpqJ5K23tvaebMEGy9+GJ26w22Uvk93nlnZfJ9SKy/7TbprrvCEka1phFbUeaB6UoAQDfKsoBqx6qcSnMPgcHOnWFk6NJLpWuv7fwAofweq894lEJQNDZWv9hsvQKcS5aEEbRGhVorX7+ditoCADBdjHhNQa2ptBdfDKNeF1+cTZtapTy9d+WVuxdvrad6GrFW3tiSJdKpp8YfwerkvDkAAOoh8JqCTlnLrtkcqerpvd25XvGKUDz15ZfHt9Z679V5Y6tWNZdwn0Yfk0MGAEgbgZfCF/Ddd8/W2rXxvoCbKcGQlankSFWfjVjJomL1L78cji8Uwu24773ZEayk+5gcMgBAFnIfeJW/gO+66xiNjcX7Au6EteymUtKhVnBkJs2fL/30p9LYWIi+SqWwUPe73y29973x3nuzI1hJ9zElLwAAWch9cn35C3j79hlNJXGXp9IuvjhcJx10lUs7xElMlyYfYaqnHBxV6umRjjsunMFYaccO6eij47/3egn3k41gJdnHU+kfAACmK/cjXp2QxD2VabGp5EjVm957z3ukm26aXr5Vu40SdkqeHgCgu+Q+8GrmCzirZOypTItNJUeqXnAkhWPvvHOXduyYMeV8q3ZaV7ET8vQAAN0n94FX+Qu4UVAx2aiTlGxANpVRuWZGmGoFlNXPu3q1dPnlW1Uqzct8tKoV2m0EDgCQD7kPvMpfwI2CinqjTqtWSV/72sSArNGSOs2a6rRYnBGmuNOYhYJ00knbNDAw9ffRbtppBA4AkA+5D7ykeEFFvVGn66/fPSC77TZp7dqwhuOsWdKb3iStWTP14CvJaTHO7gMAID0EXjHVG3UqlXYPyEql8bMOx8ZCILZqlXTaaZO/Rr0csiSnxTrh5AIAALoFgVdMtUad+vulBx4IawlOplQKI2OTBV6NpvySmhbj7D4AANJD4BVTrVGnUkl6//tb8/xZTflxdh8AAOkh8FL8JYOqR52WLw+LYzdSKIQq75PJasqPs/sAAEhP7gOvqSwZVFZvmu71r5cefDDkd5WT6xsFT1lO+XF2HwAA6WDJoCkuGSTVXgbnxBOle++VVq4MI2IrV8Y7o3EqS+oAAIDOkvsRr+lM8U02TdfsCBJTfgAAdL/cB17TneKrF2RNZXmh8nMNDoZj/+qv0l2aCAAAJCv3gVfcJYOaMZVFrVtxLAAAaG+5z/EqT/F97nNbdeml0rXXTj/IqSwN0Wze2HSOBQAA7S33gZc0vmTQxReHqb7pjixNljeW5LEAAKC9EXgloJw3Vilu3th0jgUAAO2NwKtCqRTWVFy+PFyX11ust72e6ZSGoKwEAADdK/fJ9WX1ktpvuUU69dTmkt2nUxqCshIAAHQvAq/I+vWza66V+IUv7L6G4j331F9DsbKMxPz50rx54XZ5lGzz5sYlIqgkDwBAdyLwijz0UG/NpPY77phY46u8/b77atfuqhw1e8UrxreXg6xSaXz6kBIRAADkCzlekSOPLNZMaj/wwNr779q1+7bqUhCl0sQ8sfJtSkQAAJBPBF6R/v5tNZPajz669v4zaowV1ioFUQ8lIgAAyB+mGiP1ktqHhsLIV2VA1dMjvfGNuz9HreWH6qFEBAAA+UPgVaFWUvvgoHTiibuf1VirvEO5FETcHK9GJSKmst4jAABoXwReDTRT3qF633nzwvYtW2rfnmwRbEoZomkAABFrSURBVNZsBACg+xB4xdBMeYda+5522vjtZcviBVSVifrSxIR8ykwAANCZSK5PWdxFsFmzEQCA7sOI1xSV8682bhzP4Tr++MZ5WJMFVJUjWbUS9VuVkE/uGAAA2SDwmoLK/KvqwOjEEyfPw1qwQNp774nB19577x5QVSfqT5bUP9W2kzsGAEC6mGqcgur8q7LR0caFUZcskWbOnLht5sywvVI5Uf/aa6VLLw3XrQiO4k51AgCA1iPwmoLJCqUWi9J1141Xqa+2Zo300ksTt+3cGbZXKyfqX3xxuG7FiBS5YwAAZIfAawrK+Vf1XH99mM6rFXzVCnxefLF+4FMqSatWScuXh+t6AV1ctdpOMVcAANJBjleFOEnnO3dK99wj7bFHuFSPXknS2Fj90g/NJM0nkY+VVO4YAABojMArEifI2blTes1rpOefHz9u1izphBOktWsnPl+xKF15ZbhdGcA1E/gkUcurmYKwAACgtQi8IuvXz24Y5HzhCxODLimMbr32tWEZoOpk+x/+UFq/fmIA10zgE7f0RLOaKQgLAABahxyvyEMP9TZMOr/zztrH/vd/h+Cqt3fi9npnDcZNmq+XjzVvXmvzvgAAQDoIvCJHHlncLcjZY48wvVgObE4+ufaxJ588XvrhbW+TzCY+PtWzBsvTkr294Tl7e6X+fumrX5XOOEO65JJwXS+RHwAAtBemGiP9/du0aFFInC+PfO3cGfK07rorBFYXXyx9/esTpxvLQdHQ0Hie1vr1rak4X2taslSSzjyzu9ZwpJI+ACAvMhnxMrNTzOynZvawmV2YRRuqlYOc88+fWOC0MrCZOVN6+mnpL/4ijGwdfniYTly+fHzkacmS3UeppnPWYPW05ObN3VWHq3xSAyN4AIA8SD3wMrOCpL+TNCjpWElnmNmxabejlkKhdomIysBm5sxQSf6Tnwy5XaOjE3O51qxJpuJ8WbfV4aKSPgAgT7IY8eqX9LC7P+LuOyWtkHRaBu2oKW5gM9kZh0lUnC+rlffVyXW4qKQPAMiTLHK8Dpb084r7j0taVL2TmZ0t6WxJmjNnjoaHhxNtVLFY1PDwsPbaSzrqqPn68Y/31Y4dBe25Z0lHHfWC9tprsyqbMGPGbM2adYy2bx/vwj333KVCYauGh7cl2taLLgrlLx5+uFdHHFFUf/+23eqIJaHcR62UZT8mJYl+6kb0Uzz0Uzz0Uzz0U2NJ95G5e2JPXvMFzd4jaam7fzS6/wFJ/e7+8XrHLFy40Dds2JBou4aHhzUwMCBpPNl7sjpbSVSVb3eVfdQq3diPSfRTN6Kf4qGf4qGf4qGfGmtFH5nZRndfWOuxLEa8Hpd0aMX9QyQ9mUE76opTYJQK8K1BPwIA8iSLwOteSUea2eGSnpD0Pkn/K4N2TBsV4FuDfgQA5EXqgZe77zKzcyStllSQdLW7P5B2OwAAANKWSQFVd79F0i1ZvDYAAEBWqFxfA5XUAQBAEgi8qnTjWXYAAKA9sEh2FSqpAwCApBB4VaGSOgAASAqBV5VuWwsRAAC0DwKvKtVrIc6aJR14YMj9KpWybh0AAOhkBF5VypXU//mfpcMPD9seeUQ688yQdE/wBQAAporAq4ZCIVyefVYaGwvbskqyL5WkVauk5cvDNYEfAACdi3ISdUyWZJ/W0jaUtgAAoLsw4lVHOyTZU9oCAIDuQuBVR3WSfW9vuD84mF4bKG0BAEB3YaqxjnKS/dBQCHT6+tJfOqg86lYsjm+jtAUAAJ2LwGsShULI50orp6taedStOscrzVE3AADQOgRebawdRt0AAEDrEHi1uaxH3QAAQOuQXA8AAJASAi8AAICUEHgBAACkhMALAAAgJQReAAAAKSHwAgAASEnuy0mUSqFO1g03HKZikTpZAAAgObkOvEolaenScmX4uVq5MlSGX72a4AsAALRerqcah4ZC0FUsSu6mYjHcHxrKumUAAKAb5TrwGhkJayBWGh0Ny/MAAAC0Wq4DrwULwsLTlXp6wpqIAAAArZbrwGtwMOR09fZKZq7e3nB/cDDrlgEAgG6U6+T6QiEk0g8NSTfe+KhOP/1wzmoEAACJyXXgJYUga9kyqbf3MQ0MHJ51cwAAQBfL9VQjAABAmgi8AAAAUkLgBQAAkBICLwAAgJQQeAEAAKSEwAsAACAlBF4AAAApIfACAABICYEXAABASgi8AAAAUmLunnUbGjKz/5b0WMIv8ypJv0j4NTodfRQP/RQP/RQP/RQP/RQP/dRYK/roMHd/da0HOiLwSoOZbXD3hVm3o53RR/HQT/HQT/HQT/HQT/HQT40l3UdMNQIAAKSEwAsAACAlBF7jvpV1AzoAfRQP/RQP/RQP/RQP/RQP/dRYon1EjhcAAEBKGPECAABISa4CLzN71My2mNkmM9tQ43Ezs6+a2cNmttnM3phFO7NkZkdH/VO+vGBm51XtM2Bmv6rY53NZtTdNZna1mT1rZvdXbDvAzG41s4ei61fWOfYUM/tp9Nm6ML1Wp69OP11hZj+J/l/daGb71zl20v+j3aROP33ezJ6o+L91ap1j8/55uq6ijx41s011js3F58nMDjWzH5rZVjN7wMw+EW3n71OFSfop3b9P7p6bi6RHJb1qksdPlTQkySSdKGld1m3OuL8Kkp5WqEdSuX1A0qqs25dBf7xV0hsl3V+x7XJJF0a3L5R0WZ1+/C9JvyFppqQfSTo26/eTcj8tkTQjun1ZrX6KHpv0/2g3Xer00+clfarBcbn/PFU9/n8kfa7OY7n4PEk6SNIbo9v7SHpQ0rH8fYrdT6n+fcrViFcMp0n6rgf3SNrfzA7KulEZeruk/3L3pIvXdgR3v13Sc1WbT5N0TXT7GknvrHFov6SH3f0Rd98paUV0XFeq1U/uvsbdd0V375F0SOoNazN1Pk9x5P7zVGZmJukPJV2baqPajLs/5e73Rbd/LWmrpIPF36cJ6vVT2n+f8hZ4uaQ1ZrbRzM6u8fjBkn5ecf/xaFtevU/1/6CdZGY/MrMhM/utNBvVZua4+1NS+E8t6cAa+/C5mujDCiPLtTT6P5oH50RTHlfXmRri8zTuLZKecfeH6jyeu8+Tmc2VtEDSOvH3qa6qfqqU+N+nGVM9sEOd7O5PmtmBkm41s59Ev6bKrMYxuTzt08xmSnqHpItqPHyfwvRjMcpB+TdJR6bZvg7D5ypiZp+VtEvS9+rs0uj/aLf7hqTlCp+P5QrTaB+u2ofP07gzNPloV64+T2bWK+lfJZ3n7i+EAcHGh9XY1tWfp+p+qtieyt+nXI14ufuT0fWzkm5UGGKt9LikQyvuHyLpyXRa13YGJd3n7s9UP+DuL7h7Mbp9i6Q9zOxVaTewTTxTno6Orp+tsQ+fK0lmdpakZZLe71HCRLUY/0e7mrs/4+4ld39Z0j+o9vvn8yTJzGZIepek6+rtk6fPk5ntoRBMfM/db4g28/epSp1+SvXvU24CLzPrMbN9yrcVkunur9rtZkkftOBESb8qD9PmUN1fkmb2mii3QmbWr/A52pZi29rJzZLOim6fJemmGvvcK+lIMzs8Gkl8X3RcbpjZKZI+Lekd7v5inX3i/B/talU5paer9vvP/ecp8juSfuLuj9d6ME+fp+jv8VWStrr7Vyoe4u9ThXr9lPrfp6zPMkjronDGxo+iywOSPhtt/5ikj0W3TdLfKZzhsUXSwqzbnVFf7a0QSO1Xsa2yn86J+vBHComIb8q6zSn1y7WSnpL0ksKvxI9Imi3pB5Ieiq4PiPZ9raRbKo49VeEMmv8qf/a69VKnnx5WyCPZFF2+Wd1P9f6PduulTj/9U/S3Z7PCl99BfJ5276do+3fKf5Mq9s3l50nSmxWmBzdX/B87lb9Psfsp1b9PVK4HAABISW6mGgEAALJG4AUAAJASAi8AAICUEHgBAACkhMALAAAgJQReQJczs5KZbTKz+81spZnt3eLn/5CZfb3BPgNm9qaK+x8zsw+2sh01XvMKM3vAzK6o8digmW0ws61m9hMz+3J1u6L39domX/MfzezYJvb/TTO728x2mNmnqh47xcx+amYPm9mFdY7f08yui/ZZFy2DUn7sLDN7KLqcVbH98Gjfh6JjZzbzHgFMD4EX0P22u3ufux8naadCTba0DUj6n8DL3b/p7t9N+DX/RNIb3f3PKzea2XGSvi7pTHc/RtJxkh6p0a4PKdTxic3dP+ruP27ikOcknSvpy1VtLCjUFByUdKykM+oEdB+R9Ly7HyHpSkmXRccfIOkSSYsUqmtfUrHu42WSrnT3IyU9Hz0HgJQQeAH5slbSEZJkZudHo2D3m9l50ba50QjQNRYWar6+PEJmZo+Wl4Yys4VmNlz95Gb2+9FoyoiZ/T8zmxONwnxM0iejkbe3mNnnyyM8ZtZnZvdEr3djOUAws2Ezu8zM1pvZg2b2lhqvZ9HI1v1mtsXM3httv1lSj6R15W0VLpD0RXf/iSS5+y53//vouM+b2afM7N2SFkr6XtTm3zOzGyte93fN7Iaq5y23eWF0u2hmX7SwmPw9Zjanen93f9bd71UoDlqpX9LD7v6Iu++UtELSadXHR9uuiW5fL+ntUXXupZJudffn3P15SbdKOiV67G3RvoqOfWfU3t+O3uum6N9vnxqvB2CaCLyAnLCwtt2gpC1mdrykP1IYETlR0h+b2YJo16Mlfcvd50t6QdKfNvEyd0g60d0XKAQLF7j7o5K+qTDK0ufua6uO+a6kT0evt0VhpKZshrv3SzqvanvZuyT1SXqDwhIyV5jZQe7+Do2P9FWv5XecpI2TvQl3v17SBoV12/ok3SLpGDN7dbTLH0n69mTPoRD43ePub5B0u6Q/brB/pYMVKmmXPR5tq7ufu++S9CuFauX1jp8t6ZfRvtXP+ylJ/zt6v2+RtL2J9gKIicAL6H57mdkmhUDiZwprlb1Z0o3uPuphwfMbFL5sJenn7n5ndPufo33jOkTSajPbIunPJf3WZDub2X6S9nf326JN10h6a8Uu5VGljZLm1niKN0u61sPC0s9Iuk3SCU20NxYPS3z8k6QzzWx/SSdJGmpw2E5Jq6Lb9dpfj9VqRhP7Nbtdku6U9BUzO1fh32RXjX0BTBOBF9D9yiM/fe7+8WjqqtYXcFn1F3z5/i6N/82YVefYr0n6urvPU8ixqrdfXDui65KkGTUen+x91POApOOncNy3JZ2psID8yhiByUs+viZbvfbX87ikQyvuHyLpycn2i0Y091PIG6t3/C8k7R/tO+F53f1Lkj4qaS9J95jZbzbRXgAxEXgB+XS7pHea2d5m1iPpdIX8L0l6nZmdFN0+Q2H6UJIe1XjA8gd1nnc/SU9Et8+q2P5rSbvlDLn7ryQ9X5G/9QGFUatm3sd7zawQTQO+VdL6BsdcIekzZnaUJJnZK8zs/Br7TWizuz+pEKRcrLBAc5LulXRkdAbiTEnvU1g0W2Z2jpmdE+13s8b7+d2S/jMK9lZLWmJmr4xy5pZIWh099sNoX0XH3hQ97+vdfYu7X6YwOkrgBSSAwAvIIXe/TyF4WC9pnaR/dPeR6OGtks4ys82SDpD0jWj7X0r6WzNbqzCCU8vnJa2M9vlFxfZ/l3R6Obm+6pizFHKzNivka13axFu5UdJmST+S9J8KOWVPT3aAu29WyBm71sy2Srpf0kE1dv2OpG9Gbd4r2vY9hanYZs5crMvMXmNmj0s6X9LFZva4me0bjaadoxBAbZX0fXd/IDrsNyVti25fJWm2mT0cPceF0Xt8TtJyhQDuXkmXRtsk6dOSzo+OmR09hySdF52k8COF/K5GU6kApsDGR8IB5F10BuKqqPQEqlioVzbi7lc13Dm5NqyS9K5oyhhAh2km5wAAcsvMNkoalfRnWbbD3Zdl+foApocRLwAAgJSQ4wUAAJASAi8AAICUEHgBAACkhMALAAAgJQReAAAAKSHwAgAASMn/ByUQCrCm3NtrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data to see what it looks like\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X[:, 0], y[:, 0], 'b.', markersize=10)\n",
    "plt.grid(True)\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.xlabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assumes that we can predict $y$ using a linear model (using matrix notation):\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta^Tx $$\n",
    "\n",
    "This is just a much more concise way to write:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_n x_n$$\n",
    "\n",
    "---\n",
    "#### Aside: Adding a Column of Ones to $X$ and Expressing the Hypothesis Function in Vector Form How can we write the linear hypothesis in matrix form?\n",
    "\n",
    "(Ask us for help if anything is unclear.)\n",
    "\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n. $$\n",
    "\n",
    "Recall that we can compactly write a set of linear equations as a matrix multiplication. For example\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "        a & b \\\\\n",
    "        c & d\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        e & f \\\\\n",
    "        g & h\n",
    "    \\end{bmatrix} &=\n",
    "    \\begin{bmatrix}\n",
    "        ae + bg & af + bh \\\\\n",
    "        ce + dg & cf + dh\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix}\n",
    "        i & j \\\\\n",
    "        k & l\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        m \\\\\n",
    "        n\n",
    "    \\end{bmatrix} &=\n",
    "    \\begin{bmatrix}\n",
    "        im + jn \\\\\n",
    "        km + ln\n",
    "    \\end{bmatrix}\\\\\n",
    "    \\begin{bmatrix}\n",
    "        o & p\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        q \\\\\n",
    "        r\n",
    "    \\end{bmatrix} &=\n",
    "    \\begin{bmatrix}\n",
    "        oq + pr\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "With this in mind, we can write the linear hypothesis as following:\n",
    "\\begin{align}\n",
    "    h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n =\n",
    "    \\begin{bmatrix}\n",
    "        \\theta_0 & \\theta_1 & \\cdots & \\theta_n\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        x_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_n\n",
    "    \\end{bmatrix} =\n",
    "    \\theta^T x,\n",
    "\\end{align}\n",
    "where we add a one in the $x$ vector which you can think of as the $x_0$ term:\n",
    "\\begin{align}\n",
    "    x = \\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        x_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_n\n",
    "    \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Now, the above for a single observation and we have $m$ of those. The convention is to write the matrix of inputs $X$ as a $m \\times (n + 1)$ matrix, where $n + 1$ is the number of variables *plus a constant term*:\n",
    "\\begin{align}\n",
    "    X = \\begin{bmatrix}\n",
    "        1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "        1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_1^{(m)} & \\cdots & x_n^{(m)}\n",
    "    \\end{bmatrix}.\n",
    "\\end{align}\n",
    "The first column is a column of ones such that when multiplied with the weights vector, we get the term $\\theta_0$ added for every observation:\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "        h_{\\theta}(x^{(1)}) \\\\\n",
    "        h_{\\theta}(x^{(2)}) \\\\\n",
    "        \\vdots \\\\\n",
    "        h_{\\theta}(x^{(m)})\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "        \\theta_0 + \\theta_1 x_1^{(1)} + \\cdots + \\theta_n x_n^{(1)} \\\\\n",
    "        \\theta_0 + \\theta_1 x_1^{(2)} + \\cdots + \\theta_n x_n^{(2)} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\theta_0 + \\theta_1 x_1^{(m)} + \\cdots + \\theta_n x_n^{(m)} \\\\\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "        1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "        1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_1^{(m)} & \\cdots & x_n^{(m)}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\theta_0 \\\\\n",
    "        \\theta_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\theta_n\n",
    "    \\end{bmatrix} =\n",
    "    X\\theta\n",
    "\\end{align}\n",
    "In the case of linear regression with a single variable, $X$ will have $ n + 1 = 2$ columns.\n",
    "\n",
    "For convenience, we will consider $n$ to be the number of columns of $X$ with the ones added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 4\n"
     ]
    }
   ],
   "source": [
    "X = np.insert(X, 0, 1, axis=1)\n",
    "m, n = X.shape\n",
    "print(m,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The task of training a model is therefore finding parameters $\\theta$ such that $h_{\\theta}(x)$ is as close as possible to true labels $y$. What does it mean for a prediction and a true label to be 'close'? We have to define a cost function that penalises deviations of our predictions from the truth and then proceed to search for parameters that minimize such deivations.\n",
    "\n",
    "Linear regression models are trained using a *mean squared error* cost function:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^m \\left(h_{\\theta} (x^{(i)}) - y^{(i)} \\right)^2 $$\n",
    "\n",
    "where $m$ is the total number of examples in the training data.\n",
    "\n",
    "The function above simply says: the cost is equal to the sum of errors for all observations in the training data, where each error is equal to the squared difference between the prediction and the true value of $y$. This function is continuous and yields itself to optimization via gradient-based methods. The mathematically inclined may notice that this is a convex function and hence it is possible to find a global minimum using gradient descent (in fact, linear regression has a closed form solution, but we will focus on gradient descent as a more general method of training ML models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1500\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(theta, X):  # Linear hypothesis function\n",
    "    hypothesis = np.matmul(X, theta)# YOUR CODE HERE\n",
    "    return hypothesis\n",
    "\n",
    "\n",
    "def computeCost(theta, X, y):  # Cost function\n",
    "    \"\"\"\n",
    "    theta_start is an n- dimensional vector of initial theta guess\n",
    "    X is matrix with n- columns and m- rows\n",
    "    y is a matrix with m- rows and 1 column\n",
    "    \"\"\"\n",
    "    cost = (1/(2*m)) * np.sum((h(theta,X) - y)**2)# YOUR CODE HERE\n",
    "    return cost\n",
    "\n",
    "#Test that running computeCost with 0's as theta returns 32.07:\n",
    "\n",
    "initial_theta = np.zeros((X.shape[1], 1)) #(theta is a vector with n rows and 1 columns (if X has n features) )\n",
    "print(computeCost(initial_theta, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative procedure where we update parameters in the direction of steepest descent (i.e. the direction that will reduce the cost function the most). How much we wish to move in that direction is controlled by a step size parameter $\\alpha$ (pronounced $alpha$). Hence, the formula is:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t - \\alpha  \\frac{dJ}{d\\theta} $$\n",
    "\n",
    "The gradient for the cost function can be obtained by differentiating $J$ and is equal to:\n",
    "\n",
    "$$ \\frac{dJ}{d\\theta_k} = \\frac{1}{m}\\sum_{i = 1}^m x_k^{(i)}\\left(h_{\\theta}(x^{(i)}) - y^{(i)}\\right) $$\n",
    "\n",
    "In pseudocode, we can therefore write the gradient descent algorithm as following:\n",
    "\n",
    "1. for t iterations do:\n",
    "    $$(\\theta_k)_t = (\\theta_k)_{t-1} - \\alpha  \\frac{1}{m}\\sum_{i = 1}^m x_k^{(i)}\\left(h_{\\theta}(x^{(i)}) - y^{(i)}\\right) $$\n",
    "5. end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Actual gradient descent minimizing routine\n",
    "def descendGradient(X, y, theta_start=np.zeros(2)):\n",
    "    \"\"\"\n",
    "    theta_start is an n- dimensional vector of initial theta guess\n",
    "    X is matrix with n- columns and m- rows\n",
    "    \"\"\"\n",
    "    theta = theta_start\n",
    "    \n",
    "    # Store costs over GD iterations to plot them later!\n",
    "    cost_history = []  \n",
    "    \n",
    "    # Store parameters over GD iterations to visualize how they change later\n",
    "    theta_history = []\n",
    "    for _ in range(iterations):\n",
    "        theta_new = theta\n",
    "        cost_history.append(computeCost(theta, X, y))\n",
    "\n",
    "        theta_history.append(list(theta[:, 0]))\n",
    "\n",
    "        for j in range(len(theta_new)):\n",
    "            X_j = np.array(X[:, j]).reshape(m, 1)\n",
    "            theta_new[j] = theta_new[j]- alpha * (1/m) * np.sum(X_j * (h(theta, X)-y)) # YOUR CODE HERE\n",
    "        theta = theta_new\n",
    "    return theta, theta_history, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Actually run gradient descent to get the best-fit theta values\n",
    "initial_theta = np.zeros((X.shape[1], 1))\n",
    "theta, theta_history, cost_history = descendGradient(X, y, initial_theta)\n",
    "\n",
    "# Plot the convergence of the cost function\n",
    "def plotConvergence(cost_history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(cost_history)), cost_history, 'bo')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Convergence of Cost Function\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Cost function\")\n",
    "    plt.xlim([-0.05 * iterations, 1.05 * iterations])\n",
    "\n",
    "plotConvergence(cost_history)\n",
    "#plt.ylim([4, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like at every iteration we are improving our model! That is because the cost function $J$ defined above is strictly convex and with a unique global minimum. Consequently, every iteration, provided our step size $a$ is not too large, will lead to a decrease in the cost as we improve our parameters (weights). \n",
    "\n",
    "As an aside, many ML models that you may encounter in the future do not have convex cost functions with a unique global minimum, so in general you should not expect to see the same plot as above in all circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the line on top of the data to ensure it looks correct\n",
    "def fit(xval):\n",
    "    global theta # use the parameters you trained before\n",
    "    prediction = h(theta, X) # YOUR CODE HERE\n",
    "    return prediction\n",
    "    \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X[:, 1], y[:, 0], 'rx', markersize=10, label='Training Data')\n",
    "plt.plot(X[:, 1], fit(X[:, 1]), 'b-',\n",
    "         label='Hypothesis: h(x) = %0.2f + %0.2fx' % (theta[0], theta[1]))\n",
    "plt.grid(True)  # Always plot.grid true!\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Visualizing _J($\\theta$)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you perform gradient descent to learn minimize the cost function $J(\\theta)$,\n",
    "it is helpful to monitor the convergence by computing the cost. In this\n",
    "section, you will implement a function to calculate $J(\\theta)$ so you can check the\n",
    "convergence of your gradient descent implementation.\n",
    "\n",
    "To understand the cost function $J(\\theta)$ better, you will now plot the cost over\n",
    "a 2-dimensional grid of $\\theta_0$ and $\\theta_1$ values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import necessary matplotlib tools for 3d plots\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "from matplotlib import cm\n",
    "import itertools\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "xvals = np.arange(-10, 10, .5)\n",
    "yvals = np.arange(-1, 4, .1)\n",
    "myxs, myys, myzs = [], [], []\n",
    "for i in xvals:\n",
    "    for j in yvals:\n",
    "        myxs.append(i)\n",
    "        myys.append(j)\n",
    "        myzs.append(computeCost(np.array([[i], [j]]), X, y))\n",
    "\n",
    "scat = ax.scatter(myxs, myys, myzs, c=np.abs(myzs),\n",
    "                  cmap=plt.get_cmap('YlOrRd'))\n",
    "\n",
    "plt.xlabel(r'$\\theta_0$', fontsize=30)\n",
    "plt.ylabel(r'$\\theta_1$', fontsize=30)\n",
    "plt.title('Cost (Minimization Path Shown in Blue)', fontsize=30)\n",
    "plt.plot([x[0] for x in theta_history], [x[1]\n",
    "                                        for x in theta_history], cost_history, 'bo-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. EXTRA: Linear Regression with multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with multiple variables is a straightforward extension of everything we have done up until now.\n",
    "\n",
    "The target now becomes a linear function of several variables, each of which will need to be updated by gradient descent -- but in the same manner as for a single variable regression.\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will implement linear regression with multiple variables to\n",
    "predict the prices of houses. Suppose you are selling your house and you\n",
    "want to know what a good market price would be. One way to do this is to\n",
    "first collect information on recent houses sold and make a model of housing\n",
    "prices.\n",
    "\n",
    "\n",
    "The file ex1data2.txt contains a training set of housing prices in Portland,\n",
    "Oregon. The first column is the size of the house (in square feet), the\n",
    "second column is the number of bedrooms, and the third column is the price\n",
    "of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'data/ex1data2.txt'\n",
    "# Read into the data file\n",
    "cols = np.loadtxt(datafile, delimiter=',', usecols=(0, 1, 2),\n",
    "                  unpack=True)  # Read in comma separated data\n",
    "\n",
    "# Form the usual \"X\" matrix and \"y\" vector\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "m = y.size  # number of training examples\n",
    "\n",
    "# Insert the usual column of 1's into the \"X\" matrix\n",
    "X = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quick visualize data\n",
    "plt.grid(True)\n",
    "plt.xlim([-100, 5000])\n",
    "plt.hist(X[:, 0], label='col1')\n",
    "plt.hist(X[:, 1], label='col2')\n",
    "plt.hist(X[:, 2], label='col3')\n",
    "plt.title('Clearly we need feature normalization.')\n",
    "plt.xlabel('Column Value')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the values, note that house sizes are about\n",
    "1000 times the number of bedrooms. When features differ by orders of magnitude,\n",
    "first performing feature scaling can make gradient descent converge\n",
    "much more quickly.\n",
    "\n",
    "There are various ways to normalize features. In this session, we will cover mean-variance normalization: we scale each variable such that it has mean 0 and variance 1.\n",
    "\n",
    "$$ x_j = \\frac{x_j - \\mu_j}{\\sigma_j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature normalizing the columns (subtract mean, divide by standard deviation)\n",
    "#Store the mean and std for later use\n",
    "#Note don't modify the original X matrix, use a copy\n",
    "stored_feature_means, stored_feature_stds = [], []\n",
    "Xnorm = X.copy()\n",
    "\n",
    "for icol in range(Xnorm.shape[1]):\n",
    "    stored_feature_means.append(np.mean(Xnorm[:, icol]))\n",
    "    stored_feature_stds.append(np.std(Xnorm[:, icol]))\n",
    "    # Skip the first column\n",
    "    if not icol:\n",
    "        continue\n",
    "    # Faster to not recompute the mean and std again, just used stored values\n",
    "    Xnorm[:, icol] = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quick visualize the feature-normalized data\n",
    "plt.grid(True)\n",
    "plt.xlim([-5, 5])\n",
    "plt.hist(Xnorm[:, 0], label='col1')\n",
    "plt.hist(Xnorm[:, 1], label='col2')\n",
    "plt.hist(Xnorm[:, 2], label='col3')\n",
    "plt.title('Feature Normalization Accomplished')\n",
    "plt.xlabel('Column Value')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, you implemented gradient descent on a univariate regression\n",
    "problem. The only difference now is that there is one more feature in the\n",
    "matrix X. The hypothesis function and the batch gradient descent update\n",
    "rule remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Run gradient descent with multiple variables, initial theta still set to zeros\n",
    "#(Note! This doesn't work unless we feature normalize! \"overflow encountered in multiply\")\n",
    "initial_theta = np.zeros((Xnorm.shape[1], 1))\n",
    "theta, theta_history, cost_history = descendGradient(Xnorm, y, initial_theta)\n",
    "\n",
    "#Plot convergence of cost function:\n",
    "plotConvergence(cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training Machine Learning models, **it is essential to have a test set that the model does not see in the training process**. That is because we cannot assess model performance from the data that was used to train it -- any such assessment would be misleadingly optimistic, i.e. the model would appear more accurate that it actually would be when the time arrives to predict on unseen data!\n",
    "\n",
    "We say that only unseen, or *test data*, can provide an unbiased model performance metric, such as accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the standard\n",
    "deviation used for the computations. After learning the parameters\n",
    "from the model, we often want to predict the prices of houses we have not\n",
    "seen before (i.e. the test data). Given a new x value (living room area and number of bedrooms),\n",
    "we must first normalize x using the mean and standard deviation\n",
    "*that we had previously computed from the training set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"Final result theta parameters: \\n\",theta)\n",
    "print(\"Check of result: What is price of house with 1650 square feet and 3 bedrooms?\")\n",
    "ytest = np.array([1650., 3.])\n",
    "\n",
    "\n",
    "# To \"undo\" feature normalization, we \"undo\" 1650 and 3, then plug it into our hypothesis\n",
    "ytest_scaled = [(ytest[x] - stored_feature_means[x + 1]) /\n",
    "               stored_feature_stds[x + 1] for x in range(len(ytest))]\n",
    "ytest_scaled.insert(0, 1)\n",
    "print(\"$%0.2f\" % float(h(theta, ytest_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Normal equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, linear regression is one of the few ML models whose parameters can be solved for exactly, meaning that we can avoid an iterative procedure such as gradient descent. Formally, we say that a linear regression has a closed-form solution.\n",
    "\n",
    "Let's return to matrix notation. We shall use matrix differentiation but do not worry if you do not understand some of the below - this is simply bonus material and can be equivalently derived without linear algebra.\n",
    "\n",
    "$$ H =\n",
    "    \\begin{bmatrix}\n",
    "        h_{\\theta}(x^{(1)}) \\\\\n",
    "        h_{\\theta}(x^{(2)}) \\\\\n",
    "        \\vdots \\\\\n",
    "        h_{\\theta}(x^{(m)})\n",
    "    \\end{bmatrix} = X\\theta $$\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}(y - X\\theta)^T(y - X\\theta) $$\n",
    "\n",
    "$$ \\frac{dJ}{d\\theta} = \\frac{1}{m}(X^Ty -  X^TX\\theta) $$\n",
    "\n",
    "A global minimum for a convex function is a solution to $\\frac{dJ}{d\\theta} = 0$ or: \n",
    "\n",
    "$$ X^Ty - X^TX\\theta = 0 $$\n",
    "\n",
    "$$ \\theta = (X^TX)^{-1}X^Ty $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "# Implementation of normal equation to find analytic solution to linear regression\n",
    "\n",
    "def normEqtn(X, y):\n",
    "    theta = # YOUR CODE HERE\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Normal equation prediction for price of house with 1650 square feet and 3 bedrooms\")\n",
    "print(\"$%0.2f\" % float(h(normEqtn(X, y), [1, 1650., 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
